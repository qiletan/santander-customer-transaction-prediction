{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import lightgbm as lgb\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "pd.options.display.max_columns = 500\n",
    "#import pandas_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/train.csv')\n",
    "test_df = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "np.random.seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def augmentation(x=None,y=None,upsample_times=2):\n",
    "    x_pos = x[y==1].copy()\n",
    "    y_pos = y[y==1].copy()\n",
    "    \n",
    "    aug_x_pos = x_pos.copy()\n",
    "    \n",
    "    if upsample_times ==0:\n",
    "        x_pos_temp = np.zeros(x_pos.shape)\n",
    "        for i in range(x_pos.shape[1]):\n",
    "            pos_values = x_pos.iloc[:,i].values\n",
    "            np.random.shuffle(pos_values)\n",
    "            x_pos_temp[:,i] = pos_values\n",
    "\n",
    "        x_pos_temp = pd.DataFrame(x_pos_temp)\n",
    "        x_pos_temp.columns = x_pos.columns  \n",
    "        aug_x_pos = x_pos_temp    \n",
    "        aug_y_pos = np.ones(y_pos.shape[0]) \n",
    "        aug_x_pos = aug_x_pos.append(x[y==0]) \n",
    "        aug_y_pos = np.append(aug_y_pos,y[y==0]) \n",
    "    else:\n",
    "        for n in range(upsample_times):\n",
    "            x_pos_temp = np.zeros(x_pos.shape)\n",
    "\n",
    "            for i in range(x_pos.shape[1]):\n",
    "                pos_values = x_pos.iloc[:,i].values\n",
    "                np.random.shuffle(pos_values)\n",
    "                x_pos_temp[:,i] = pos_values\n",
    "\n",
    "            x_pos_temp = pd.DataFrame(x_pos_temp)\n",
    "            x_pos_temp.columns = x_pos.columns  \n",
    "            aug_x_pos = aug_x_pos.append(x_pos_temp)    \n",
    "\n",
    "        aug_y_pos = np.ones(y_pos.shape[0]*(upsample_times+1)) \n",
    "\n",
    "        aug_x_pos = aug_x_pos.append(x[y==0]) \n",
    "        aug_y_pos = np.append(aug_y_pos,y[y==0]) \n",
    "        \n",
    "    return aug_x_pos,aug_y_pos\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df[var_list] = np.exp(train_df[var_list])\n",
    "# test_df[var_list] = np.exp(test_df[var_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: augment_3_times_5_folds\n"
     ]
    }
   ],
   "source": [
    "n_folds = 5\n",
    "random_seed = 26\n",
    "upsample_times = 3\n",
    "model = f'augment_{upsample_times}_times'\n",
    "\n",
    "model_name = \"{0}_{1}_folds\".format(model, n_folds)\n",
    "print(\"Model: {}\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exclusion = ['ID_code', 'target']+ rank_var_list\n",
    "exclusion = ['ID_code', 'target'] \n",
    "feats = [c for c in train_df.columns if c not in exclusion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Fold: 1\n"
     ]
    },
    {
     "ename": "LightGBMError",
     "evalue": "GPU Tree Learner was not enabled in this build.\nPlease recompile with CMake option -DUSE_GPU=1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-952e2f05078d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m                      \u001b[0mnum_boost_round\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m                      \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m                      verbose_eval=False)\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;31m# construct booster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m         \u001b[0mbooster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBooster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_train_data_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, train_set, model_file, silent)\u001b[0m\n\u001b[0;32m   1552\u001b[0m                 \u001b[0mtrain_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m                 \u001b[0mc_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1554\u001b[1;33m                 ctypes.byref(self.handle)))\n\u001b[0m\u001b[0;32m   1555\u001b[0m             \u001b[1;31m# save reference to data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1556\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_safe_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \"\"\"\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecode_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLGBM_GetLastError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLightGBMError\u001b[0m: GPU Tree Learner was not enabled in this build.\nPlease recompile with CMake option -DUSE_GPU=1"
     ]
    }
   ],
   "source": [
    "clfs = []\n",
    "folds = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_seed)\n",
    "oof_preds = np.zeros((len(train_df), 1))\n",
    "test_preds = np.zeros((len(test_df), 1))\n",
    "\n",
    "\n",
    "X = train_df[feats]\n",
    "y = train_df['target']\n",
    "X_test = test_df[feats]\n",
    "test_ids = test_df.ID_code.values\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'is_unbalance': 'true',\n",
    "    #'scale_pos_weight': 400,\n",
    "    #'device' : 'gpu' ,\n",
    "    'boosting': 'gbdt',\n",
    "    'num_leaves': 31, #31\n",
    "    'feature_fraction': 0.5,\n",
    "    'bagging_fraction': 0.7,\n",
    "    'bagging_freq': 10,\n",
    "    'learning_rate': 0.05, #0.05\n",
    "    'verbose': 30\n",
    "    #'min_data_in_leaf': 200\n",
    "}\n",
    "\n",
    "feature_importance_df = pd.DataFrame()\n",
    "for fold_, (trn_, val_) in enumerate(folds.split(X, y)):\n",
    "    print(\"Current Fold: {}\".format(fold_+1))\n",
    "    trn_x, trn_y = X.iloc[trn_, :], y[trn_]\n",
    "    val_x, val_y = X.iloc[val_, :], y[val_]\n",
    "    trn_x, trn_y = augmentation(trn_x,trn_y,upsample_times)\n",
    "\n",
    "    trn_lgb = lgb.Dataset(trn_x, trn_y)\n",
    "    val_lgb = lgb.Dataset(val_x, val_y)\n",
    "    clf = lgb.train(parameters,\n",
    "                     train_set=trn_lgb,\n",
    "                     #valid_sets=[valid_data_lgb,holdout_data_lgb],\n",
    "                     valid_sets=[trn_lgb, val_lgb],\n",
    "                     num_boost_round=3000,\n",
    "                     early_stopping_rounds=50,\n",
    "                     verbose_eval=False)\n",
    "    \n",
    "\n",
    "\n",
    "    val_pred = clf.predict(val_x)\n",
    "    test_fold_pred = clf.predict(X_test)\n",
    "\n",
    "    print(\"AUC = {}\".format(roc_auc_score(val_y, val_pred)))\n",
    "    oof_preds[val_, :] = val_pred.reshape((-1, 1))\n",
    "    test_preds += test_fold_pred.reshape((-1, 1))\n",
    "    \n",
    "   # print('getting feature importance')\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = feats\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    \n",
    "test_preds /= n_folds\n",
    "roc_score = roc_auc_score(y, oof_preds.ravel())\n",
    "print(\"Overall AUC = {}\".format(roc_score))\n",
    "\n",
    "\n",
    "print(\"Saving submission file\")\n",
    "sample = pd.read_csv('../data/sample_submission.csv')\n",
    "sample.target = test_preds.astype(float)\n",
    "sample.ID_code = test_ids\n",
    "sample.to_csv('../submissions/{}_{}.csv'.format(model_name,str(roc_score)), index=False)\n",
    "\n",
    "#display_importances(feature_importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skopt functions\n",
    "from skopt import BayesSearchCV\n",
    "from skopt import gp_minimize # Bayesian optimization using Gaussian Processes\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.utils import use_named_args # decorator to convert a list of parameters to named arguments\n",
    "from skopt.callbacks import DeadlineStopper # Stop the optimization before running out of a fixed budget of time.\n",
    "from skopt.callbacks import VerboseCallback # Callback to control the verbosity\n",
    "from skopt.callbacks import DeltaXStopper # Stop the optimization If the last two positions at which the objective has been evaluated are less than delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reporting util for different optimizers\n",
    "def report_perf(optimizer, X, y, title, callbacks=None):\n",
    "    \"\"\"\n",
    "    A wrapper for measuring time and performances of different optmizers\n",
    "    \n",
    "    optimizer = a sklearn or a skopt optimizer\n",
    "    X = the training set \n",
    "    y = our target\n",
    "    title = a string label for the experiment\n",
    "    \"\"\"\n",
    "    start = time()\n",
    "    if callbacks:\n",
    "        optimizer.fit(X, y, callback=callbacks)\n",
    "    else:\n",
    "        optimizer.fit(X, y)\n",
    "    best_score = optimizer.best_score_\n",
    "    best_score_std = optimizer.cv_results_['std_test_score'][optimizer.best_index_]\n",
    "    best_params = optimizer.best_params_\n",
    "    print((title + \" took %.2f seconds,  candidates checked: %d, best CV score: %.3f \"\n",
    "           +u\"\\u00B1\"+\" %.3f\") % (time() - start, \n",
    "                                  len(optimizer.cv_results_['params']),\n",
    "                                  best_score,\n",
    "                                  best_score_std))    \n",
    "    print('Best parameters:')\n",
    "    pprint.pprint(best_params)\n",
    "    print()\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_auc_score, mean_absolute_error\n",
    "from sklearn.metrics import make_scorer\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = make_scorer(roc_auc_score, greater_is_better=True, needs_proba=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_seed)\n",
    "\n",
    "clf = lgb.LGBMClassifier(boosting_type='gbdt',\n",
    "                         #is_unbalance ='true',\n",
    "                         objective='binary',\n",
    "                         #n_jobs=1, \n",
    "                         verbose=0)\n",
    "\n",
    "\n",
    "search_spaces = {\n",
    "        'learning_rate': Real(0.01, 1.0, 'log-uniform'),\n",
    "        'num_leaves': Integer(2, 500),\n",
    "        'max_depth': Integer(0, 500),\n",
    "        'min_child_samples': Integer(0, 200),\n",
    "        'max_bin': Integer(100, 100000),\n",
    "        'subsample': Real(0.01, 1.0, 'uniform'),\n",
    "        'subsample_freq': Integer(0, 10),\n",
    "        'colsample_bytree': Real(0.01, 1.0, 'uniform'),\n",
    "        'min_child_weight': Integer(0, 10),\n",
    "        'subsample_for_bin': Integer(100000, 500000),\n",
    "        'reg_lambda': Real(1e-9, 1000, 'log-uniform'),\n",
    "        'reg_alpha': Real(1e-9, 1.0, 'log-uniform'),\n",
    "        'scale_pos_weight': Real(1e-6, 500, 'log-uniform'),\n",
    "        'n_estimators': Integer(500, 30000)        \n",
    "        }\n",
    "\n",
    "opt = BayesSearchCV(clf,\n",
    "                    search_spaces,\n",
    "                    scoring=auc,\n",
    "                    cv=folds,\n",
    "                    n_iter=40,\n",
    "                    n_jobs=-1,\n",
    "                    return_train_score=False,\n",
    "                    refit=True,\n",
    "                    optimizer_kwargs={'base_estimator': 'GP'},\n",
    "                    random_state=22)\n",
    "X = train_df[feats]\n",
    "y = train_df['target']    \n",
    "best_params = report_perf(opt, X, y,'LightGBM', \n",
    "                          callbacks=[DeltaXStopper(0.001), \n",
    "                                     DeadlineStopper(60*5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
